{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "# Set shell to show all lines of output\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "books = []\n",
    "# with open('../data/found_books_filtered.ndjson', 'r') as fin:\n",
    "with open(\"/Users/jun_yu/Documents/yujun_repository/wikipedia-data-science-master/data/found_books_filtered.ndjson\", 'r') as fin:\n",
    "    books = [json.loads(l) for l in fin]  # Append each line to the books  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wikipedia:Wikipedia Signpost/2014-06-25/Recent research', {'name': 'Global Wikipedia', 'author': 'Pnina Fichman and Noriko Hara', 'country': 'United States', 'language': 'English', 'subject': 'Wikipedia', 'publisher': 'Rowman  &  Littlefield', 'release_date': '2014', 'pages': '178', 'isbn': '978-0810891012'}, ['User:Adler.fa', 'User:Maximilianklein', 'User:Piotrus', 'User:Kimaus', 'User:Tbayer (WMF)', 'Rowman  &  Littlefield', 'Indiana University Bloomington', 'User:Maximilianklein', 'User talk:Maximilianklein', 'File:Immanuel Kant (painted portrait).jpg', 'Immanuel Kant', 'PageRank', 'CheiRank', 'm:Research:Newsletter/2013/April#How_Wikipedia.27s_Google_matrix_differs_for_politicians_and_artists', 'm:Research:Newsletter/2013/July#Multilingual_ranking_analysis:_Napoleon_and_Michael_Jackson_as_Wikipedia.27s_.22global_heroes.22', 'DBpedia', 'User:Piotrus', 'OpenSym', 'Chinese Wikipedia', 'Baidu Baike', 'microblog', 'Twitter', 'Sina Weibo', 'Internet censorship in China', 'User:Kimaus', 'Critical mass (sociodynamics)', 'File:Vis gartenbaukino3.jpg', 'Bloomington, Indiana', 'Wikipedia:Wikipedia_Signpost/2012-11-26/Recent_research', 'File:Belgium provinces regions striped.png', 'Flemish Community', 'French Community of Belgium', 'German-speaking Community of Belgium', 'm:Research:Newsletter#How to contribute', 'EHEC', 'Category:Wikipedia Signpost archives 2014-06', 'Category:Wikipedia Signpost Research report archives 2014'], ['http://nbviewer.ipython.org/github/notconfusing/wikidataSex/blob/master/ratio_analysis/Sex%20Ratios%20By%20Language%20May%202014.ipynb', 'http://hci.cs.umanitoba.ca/assets/publication_files/IntelWiki_UMAP_final.pdf', 'http://hci.cs.umanitoba.ca/assets/publication_files/Mohammad_Noor_Nawaz_Thesis.pdf', 'http://hci.cs.umanitoba.ca/projects-and-research/details/intelwiki', 'https://www.youtube.com/watch?v=8X8aTdnYBRI', 'http://people.oii.ox.ac.uk/hanteng/2014/06/17/what-do-chinese-language-microblog-users-do-with-baidu-baike-and-chinese-wikipedia/', 'http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8104', 'http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0071226', 'http://journal.aall.org.au/index.php/jall/article/view/319', 'http://people.oii.ox.ac.uk/hanteng/2014/05/13/how-does-wikipedia-cover-the-world-differently-than-google-or-bing-2/', 'http://dx.doi.org/10.1080/09298215.2013.848904', 'http://www.skynet.ie/~arash/PDFs/JIS-2487-accepted.pdf', 'http://link.springer.com/article/10.1007/s11616-013-0191-z', 'http://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=3523'], '2017-04-12T15:59:38Z', 18267]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(books)):\n",
    "    if index <500:\n",
    "        if 'Wikipedia:' in books[index][0]:\n",
    "            print(books[index])\n",
    "            print(type(books[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37020 books.\n",
      "Found 20 books.\n"
     ]
    }
   ],
   "source": [
    "## Remove non-book articles\n",
    "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]]\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]]\n",
    "print(f'Found {len(books)} books.')                 ## wiki book \n",
    "print(f'Found {len(books_with_wikipedia)} books.')  ## none wiki book "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia:Wikipedia Signpost/2014-06-25/Recent research',\n",
       " 'Wikipedia:New pages patrol/Unpatrolled articles/December 2010',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 September 23',\n",
       " 'Wikipedia:Articles for creation/Redirects/2012-10',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 October 4']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wiki book detail \n",
    "# 只有20个, 且被举例 \n",
    "[book[0] for book in books_with_wikipedia][:5]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('Limonov (novel)',\n",
       " {'author': 'Emmanuel Carrère',\n",
       "  'country': 'France',\n",
       "  'english_pub_date': '2014',\n",
       "  'isbn': '978-2-8180-1405-9',\n",
       "  'language': 'French',\n",
       "  'name': 'Limonov',\n",
       "  'pages': '488',\n",
       "  'pub_date': '2011',\n",
       "  'publisher': 'P.O.L.',\n",
       "  'translator': 'John Lambert'},\n",
       " ['Emmanuel Carrère', 'biographical novel', 'Emmanuel Carrère'],\n",
       " ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php',\n",
       "  'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html',\n",
       "  'http://limonow.de/carrere/index.html'],\n",
       " '2018-08-18T02:03:21Z',\n",
       " 1437)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## none wiki book detail \n",
    "## the 21th books, contain 6 items\n",
    "# title, the information from the Infobox book template, the internal wikipedia links, \n",
    "# the external links, the date of last edit, and the number of characters in the article\n",
    "# 其中两个item（links) 是列表list \n",
    "n = 21 \n",
    "len(books[n])   \n",
    "books[n][0], books[n][1], books[n][2][:3], books[n][3][:3], books[n][4], books[n][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 将book 编号 \n",
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "book_index['The Book of Night Women']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna Karenina'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 将book reverse 编号 \n",
    "index_book = {idx: book for book, idx in book_index.items()}\n",
    "index_book[22494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 将所有book的 internal wiki link 列表合并成 一张list  \n",
    "from itertools import chain\n",
    "wikilinks = list(chain(*[book[2] for book in books]))  ## chain 将多张list合并, 未去重  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1187773"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311276 unique internal link\n"
     ]
    }
   ],
   "source": [
    "len(wikilinks)\n",
    "type(wikilinks)\n",
    "print(f'{len(set(wikilinks))} unique internal link')  # 去重的internal link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]  ## 是wikilinks 且在book中, 未去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50407"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17032 unique wikilinks to other books\n"
     ]
    }
   ],
   "source": [
    "len(wikilinks_other_books)\n",
    "type(wikilinks_other_books)\n",
    "print(f'{len(set(wikilinks_other_books))} unique wikilinks to other books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    # Create a counter object\n",
    "    counts = Counter(l)  # list to value counts\n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)  # list tuple\n",
    "    counts = OrderedDict(counts)   \n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find set of wikilinks for each book and convert to a flattened list \n",
    "# 获取每个book 的去重wikilinks, 并合并成 flatten list;\n",
    "# 单个用户去重 \n",
    "unique_wikilinks = list(chain(*[list(set(book[2])) for book in books]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030651"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311276 unique internal link\n"
     ]
    }
   ],
   "source": [
    "len(unique_wikilinks)\n",
    "type(unique_wikilinks)\n",
    "print(f'{len(set(unique_wikilinks))} unique internal link') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7489),\n",
       " ('Paperback', 7311),\n",
       " ('Wikipedia:WikiProject Books', 6043),\n",
       " ('Wikipedia:WikiProject Novels', 6015),\n",
       " ('English language', 4185),\n",
       " ('United States', 3060),\n",
       " ('Science fiction', 3030),\n",
       " ('The New York Times', 2727),\n",
       " ('science fiction', 2502),\n",
       " ('novel', 1979)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts = count_items(unique_wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 297624 unique wikilinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('paperback', 8740),\n",
       " ('hardcover', 8648),\n",
       " ('wikipedia:wikiproject books', 6043),\n",
       " ('wikipedia:wikiproject novels', 6016),\n",
       " ('science fiction', 5665),\n",
       " ('english language', 4248),\n",
       " ('united states', 3063),\n",
       " ('novel', 2983),\n",
       " ('the new york times', 2742),\n",
       " ('fantasy', 2003)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_remove = ['hardcover','paperback', 'science fiction','english language','united states',\n",
    "             'wikipedia:wikiproject books', 'wikipedia:wikiproject novels','novel','the new york times']\n",
    "for t in to_remove:\n",
    "    wikilinks.remove(t)\n",
    "    _ = wikilink_counts.pop(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41755\n"
     ]
    }
   ],
   "source": [
    "# Limit to greater than 3 links\n",
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 4]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Linked-to Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Encyclopedia of Science Fiction', 127),\n",
       " ('The Discontinuity Guide', 104),\n",
       " ('The Encyclopedia of Fantasy', 63),\n",
       " ('Dracula', 55),\n",
       " ('Encyclopædia Britannica', 51),\n",
       " ('Nineteen Eighty-Four', 51),\n",
       " ('Don Quixote', 49),\n",
       " ('The Wonderful Wizard of Oz', 49),\n",
       " (\"Alice's Adventures in Wonderland\", 47),\n",
       " ('Jane Eyre', 39)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of book wikilinks for each book\n",
    "unique_wikilinks_books = list(chain(*[list(set(link for link in book[2] if link in book_index.keys())) for book in books]))\n",
    "\n",
    "# Count the number of books linked to by other books\n",
    "wikilink_book_counts = count_items(unique_wikilinks_books)\n",
    "list(wikilink_book_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Big Picture: Who Killed Hollywood? and Other Essays ['Wikipedia:WikiProject Novels', 'Wikipedia:WikiProject Books', 'William Goldman', 'United States', 'English language', 'William Goldman', 'Michael Sragow', 'Good Will Hunting', 'Robin Williams', 'Matt Damon', 'The New York Times', 'The New York Times Company', 'New York Times', 'Category:Cinema of the United States', 'Category:Film production', 'Category:2000 books', 'Category:Books about films', 'Category:Books by William Goldman', 'Category:Show business memoirs']\n"
     ]
    }
   ],
   "source": [
    "for book in books:\n",
    "    if 'The New York Times' in book[2] and 'New York Times' in book[2]:\n",
    "        print(book[0], book[2])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts.get('the new york times')\n",
    "wikilink_counts.get('new york times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikilinks to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"category:children's novels about animals\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41755 wikilinks that will be used.\n"
     ]
    }
   ],
   "source": [
    "link_index['the economist']\n",
    "index_link[300]\n",
    "print(f'There are {len(link_index)} wikilinks that will be used.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "41755\n"
     ]
    }
   ],
   "source": [
    "print(type(link_index))\n",
    "print(len(link_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "# Iterate through each book\n",
    "for book in books:\n",
    "    # Iterate through the links in the book\n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(753913, 41755, 37020)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs), len(links), len(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640636"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator For Training Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Adjust label based on task\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1\n",
    "    \n",
    "    # This creates a generator\n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "\n",
    "        # Increment idx by 1\n",
    "        idx += 1\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs_set:\n",
    "                \n",
    "                # Add to batch and increment index\n",
    "                batch[idx, :] = (random_book, random_link, neg_label)\n",
    "                idx += 1\n",
    "                \n",
    "        # Make sure to shuffle order\n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'book': array([22458., 29814.,  7206., 25757.,  7015., 28410.]),\n",
       "  'link': array([1.5000e+01, 1.1452e+04, 3.4924e+04, 2.2920e+04, 1.3820e+04,\n",
       "         3.3217e+04])},\n",
       " array([ 1., -1., -1., -1.,  1., -1.]))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'book': array([ 4106., 20277.,  3138., 21977., 29880.,  6116.]),\n",
       "  'link': array([ 2724., 13360., 13401., 15053., 17263.,  4812.])},\n",
       " array([ 1., -1., -1., -1., -1.,  1.]))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 4106.0 2724.0\n",
      "-1.0 20277.0 13360.0\n",
      "-1.0 3138.0 13401.0\n",
      "-1.0 21977.0 15053.0\n",
      "-1.0 29880.0 17263.0\n",
      "1.0 6116.0 4812.0\n"
     ]
    }
   ],
   "source": [
    "for label, b_idx, l_idx in zip(y, x['book'], x['link']):\n",
    "    print(label, b_idx, l_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: New Moon (novel)               Link: british columbia                         Label: 1.0\n",
      "Book: Soul Music (novel)             Link: babylonia                                Label: -1.0\n",
      "Book: The Soul of the Robot          Link: category:novels about rebels             Label: -1.0\n",
      "Book: The Counterfeit Man            Link: geneticist                               Label: -1.0\n",
      "Book: Des Imagistes                  Link: category:temeraire books                 Label: -1.0\n",
      "Book: The Haunting of Hill House     Link: shirley jackson                          Label: 1.0\n"
     ]
    }
   ],
   "source": [
    "# x, y = next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))\n",
    "\n",
    "# Show a few example training pairs\n",
    "for label, b_idx, l_idx in zip(y, x['book'], x['link']):\n",
    "    print(f'Book: {index_book[b_idx]:30} Link: {index_link[l_idx]:40} Label: {label}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def book_embedding_model(embedding_size = 50, classification = False):\n",
    "    \"\"\"Model to embed books and wikilinks using the functional API.\n",
    "       Trained to discern if a link is present in a article\"\"\"\n",
    "    \n",
    "    # Both inputs are 1-dimensional\n",
    "    book = tf.keras.Input(name = 'book', shape = [1])\n",
    "    link = tf.keras.Input(name = 'link', shape = [1])\n",
    "    \n",
    "    # Embedding the book (shape will be (None, 1, 50))\n",
    "    book_embedding = tf.keras.layers.Embedding(name = 'book_embedding',\n",
    "                                               input_dim = len(book_index),\n",
    "                                               output_dim = embedding_size)(book)\n",
    "    \n",
    "    # Embedding the link (shape will be (None, 1, 50))\n",
    "    link_embedding = tf.keras.layers.Embedding(name = 'link_embedding',\n",
    "                                               input_dim = len(link_index),\n",
    "                                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    # Merge the layers with a dot product along the second axis (shape will be (None, 1, 1))\n",
    "    merged = tf.keras.layers.Dot(name = 'dot_product', normalize = True, axes = 2)([book_embedding, link_embedding])\n",
    "    \n",
    "    # Reshape to be a single number (shape will be (None, 1))\n",
    "    merged = tf.keras.layers.Reshape(target_shape = [1])(merged)\n",
    "    \n",
    "    # If classifcation, add extra layer and loss function is binary cross entropy\n",
    "    if classification:\n",
    "        merged = tf.keras.layers.Dense(1, activation = 'sigmoid')(merged)\n",
    "        model = tf.keras.Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    # Otherwise loss function is mean squared error\n",
    "    else:\n",
    "        model = tf.keras.Model(inputs = [book, link], outputs = merged)\n",
    "        model.compile(optimizer = 'Adam', loss = 'mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "book (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_embedding (Embedding)      (None, 1, 50)        1851000     book[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 50)        2087750     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1, 1)         0           book_embedding[0][0]             \n",
      "                                                                 link_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1)            0           dot_product[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 3,938,750\n",
      "Trainable params: 3,938,750\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model and show parameters\n",
    "model = book_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "736/736 - 128s - loss: 1.0228\n",
      "Epoch 2/15\n",
      "736/736 - 119s - loss: 1.0221\n",
      "Epoch 3/15\n",
      "736/736 - 115s - loss: 1.0081\n",
      "Epoch 4/15\n",
      "736/736 - 115s - loss: 0.9510\n",
      "Epoch 5/15\n",
      "736/736 - 116s - loss: 0.8886\n",
      "Epoch 6/15\n",
      "736/736 - 115s - loss: 0.8668\n",
      "Epoch 7/15\n",
      "736/736 - 116s - loss: 0.8598\n",
      "Epoch 8/15\n",
      "736/736 - 115s - loss: 0.8577\n",
      "Epoch 9/15\n",
      "736/736 - 116s - loss: 0.8615\n",
      "Epoch 10/15\n",
      "736/736 - 116s - loss: 0.8674\n",
      "Epoch 11/15\n",
      "736/736 - 116s - loss: 0.8655\n",
      "Epoch 12/15\n",
      "736/736 - 118s - loss: 0.8598\n",
      "Epoch 13/15\n",
      "736/736 - 120s - loss: 0.8603\n",
      "Epoch 14/15\n",
      "736/736 - 120s - loss: 0.8576\n",
      "Epoch 15/15\n",
      "736/736 - 116s - loss: 0.8551\n"
     ]
    }
   ],
   "source": [
    "n_positive = 1024\n",
    "gen = generate_batch(pairs, n_positive, negative_ratio = 2)\n",
    "\n",
    "# Train\n",
    "h = model.fit_generator(gen, epochs = 15, \n",
    "                        steps_per_epoch = len(pairs) // n_positive,\n",
    "                        verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37020, 50)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract embeddings\n",
    "book_layer = model.get_layer('book_embedding')\n",
    "book_weights = book_layer.get_weights()[0]\n",
    "book_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6900416 ],\n",
       "       [0.70101345],\n",
       "       [0.6896557 ],\n",
       "       ...,\n",
       "       [0.6942998 ],\n",
       "       [0.7056162 ],\n",
       "       [0.6906975 ]], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_weights = book_weights / np.linalg.norm(book_weights, axis = 1).reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00763809, -0.10855465, -0.26320764,  0.03021353, -0.11994871,\n",
       "       -0.14653577, -0.08622703, -0.027542  ,  0.18512689, -0.20153804],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_weights[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.square(book_weights[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.manifold import TSNE \n",
    "# from umap import UMAP\n",
    "from umap.umap_ import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02809879, -0.08677204, -0.13547102,  0.07176922,  0.04170844,\n",
       "       -0.07650553, -0.09600848, -0.23653129,  0.13618682,  0.02368179,\n",
       "        0.12479047,  0.10791729, -0.15140153,  0.05245212, -0.0279427 ,\n",
       "        0.14660397, -0.25109342, -0.04736791, -0.05580385,  0.25056913,\n",
       "        0.0427866 ,  0.10776127,  0.16093065, -0.21726526, -0.0844913 ,\n",
       "        0.11647005,  0.12308263, -0.11582815, -0.15723653, -0.09709873,\n",
       "       -0.18291065, -0.19662352, -0.02255392, -0.07425059,  0.095594  ,\n",
       "       -0.08978405,  0.16337256,  0.3433822 , -0.03025817,  0.11550695,\n",
       "       -0.06635606, -0.14766791,  0.11871085,  0.2899773 ,  0.0399634 ,\n",
       "       -0.22708412, -0.12412411, -0.04168186,  0.18877849, -0.12432928],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_vector = np.matrix(book_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.00763809, -0.10855465, -0.26320764, ...,  0.13652009,\n",
       "         -0.10672116, -0.12862533],\n",
       "        [ 0.02809879, -0.08677204, -0.13547102, ..., -0.04168186,\n",
       "          0.18877849, -0.12432928],\n",
       "        [ 0.06197103,  0.17694925, -0.2694963 , ...,  0.18984146,\n",
       "          0.05716776,  0.12006257],\n",
       "        ...,\n",
       "        [-0.03111789, -0.14378302, -0.1933183 , ..., -0.11054443,\n",
       "          0.03896669,  0.03428557],\n",
       "        [-0.03612754,  0.0183298 , -0.00701046, ..., -0.3415668 ,\n",
       "          0.21962376, -0.02534804],\n",
       "        [-0.07679123, -0.22202146, -0.2950644 , ..., -0.00695259,\n",
       "          0.10289907, -0.09963502]], dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tm_vector = 1.-tmp_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.00763809, -0.10855465, -0.26320764, ...,  0.13652009,\n",
       "         -0.10672116, -0.12862533],\n",
       "        [ 0.02809879, -0.08677204, -0.13547102, ..., -0.04168186,\n",
       "          0.18877849, -0.12432928],\n",
       "        [ 0.06197103,  0.17694925, -0.2694963 , ...,  0.18984146,\n",
       "          0.05716776,  0.12006257],\n",
       "        ...,\n",
       "        [-0.03111789, -0.14378302, -0.1933183 , ..., -0.11054443,\n",
       "          0.03896669,  0.03428557],\n",
       "        [-0.03612754,  0.0183298 , -0.00701046, ..., -0.3415668 ,\n",
       "          0.21962376, -0.02534804],\n",
       "        [-0.07679123, -0.22202146, -0.2950644 , ..., -0.00695259,\n",
       "          0.10289907, -0.09963502]], dtype=float32)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X should be a square distance matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-aaa429be5d8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    644\u001b[0m                                  \"used with metric=\\\"precomputed\\\".\")\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X should be a square distance matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 raise ValueError(\"All distances should be positive, the \"\n",
      "\u001b[0;31mValueError\u001b[0m: X should be a square distance matrix"
     ]
    }
   ],
   "source": [
    "weights = tmp_vector \n",
    "TSNE(n_components=3, metric = 'precomputed').fit_transform(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_dim(weights, components = 3, method = 'tsne'):\n",
    "    \"\"\"\n",
    "    Reduce dimensions of embeddings\n",
    "    \"\"\"\n",
    "    if method == 'tsne':\n",
    "        return TSNE(n_components=components, \n",
    "                    metric = 'precomputed').fit_transform(weights)\n",
    "    elif method == 'umap':\n",
    "        # Might want to try different parameters for UMAP\n",
    "        return UMAP(n_components = components, \n",
    "                    metric = 'precomputed', \n",
    "                    init = 'random', \n",
    "                    n_neighbors = 5).fit_transform(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X should be a square distance matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-213-41386dc10b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m book_r = reduce_dim(tm_vector, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mcomponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     method ='tsne')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-211-996b2d9d16a9>\u001b[0m in \u001b[0;36mreduce_dim\u001b[0;34m(weights, components, method)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tsne'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         return TSNE(n_components=components, \n\u001b[0;32m----> 7\u001b[0;31m                     metric = 'precomputed').fit_transform(weights)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'umap'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Might want to try different parameters for UMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \"\"\"\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    644\u001b[0m                                  \"used with metric=\\\"precomputed\\\".\")\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X should be a square distance matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m                 raise ValueError(\"All distances should be positive, the \"\n",
      "\u001b[0;31mValueError\u001b[0m: X should be a square distance matrix"
     ]
    }
   ],
   "source": [
    "book_r = reduce_dim(tm_vector, \n",
    "                    components = 2, \n",
    "                    method ='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open(\"/Users/jun_yu/Downloads/recall_videos.json\", 'r') as file:\n",
    "#     json_data = file.read()\n",
    "\n",
    "# data = json.loads(json_data)\n",
    "# print(type(data))\n",
    "\n",
    "# for i in data:\n",
    "#     print(type(i))\n",
    "#     for k,v in i.items():\n",
    "#         if k == 'recall_list':\n",
    "#             print(type(v))\n",
    "# #             print(v.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
